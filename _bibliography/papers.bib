@article{zhang2023generative,
  title={Generative gradient inversion without prior},
  author={Chi Zhang and Xiaoman Zhang and Ekanut Sotthiwat and Yanyu Xu and Ping Liu and Liangli Zhen* and Yong Liu},
  journal={ICCV},
  year={2023},
  abstract = {Federated learning has gained recognitions as a secure approach for safeguarding local private data in collaborative learning. But the advent of gradient inversion research has posed significant challenges to this premise by enabling a third-party to recover groundtruth images via gradients. While prior research has predominantly focused on low-resolution images and small batch sizes, this study highlights the feasibility of reconstructing complex images with high resolutions and large batch sizes. The success of the proposed method is contingent on three crucial components: a convolutional generative model, an over-parameterized network, and a well-designed architecture. Practical experiments demonstrate that the proposed algorithm achieves high-fidelity image recovery, surpassing state-of-the-art competitors that commonly fail in more intricate scenarios. Consequently, our study shows that local participants in a federated learning system are vulnerable to potential data leakage issues. The source code will be available upon publication.}
  preview={GGI_2013.png}
}

@article{feng2023contrastive,
  title={Contrastive domain adaptation with consistency match for automated pneumonia diagnosis},
  author={Feng, Yangqin and Wang, Zizhou and Xu, Xinxing and Wang, Yan and Fu, Huazhu and Li, Shaohua and Zhen, Liangli and Lei, Xiaofeng and Cui, Yingnan and Ting, Jordan Sim Zheng and others},
  journal={Medical Image Analysis},
  volume={83},
  pages={102664},
  year={2023},
  abstract = {Pneumonia can be difficult to diagnose since its symptoms are too variable, and the radiographic signs are often very similar to those seen in other illnesses such as a cold or influenza. Deep neural networks have shown promising performance in automated pneumonia diagnosis using chest X-ray radiography, allowing mass screening and early intervention to reduce the severe cases and death toll. However, they usually require many well-labelled chest X-ray images for training to achieve high diagnostic accuracy. To reduce the need for training data and annotation resources, we propose a novel method called Contrastive Domain Adaptation with Consistency Match (CDACM). It transfers the knowledge from different but relevant datasets to the unlabelled small-size target dataset and improves the semantic quality of the learnt representations. Specifically, we design a conditional domain adversarial network to exploit discriminative information conveyed in the predictions to mitigate the domain gap between the source and target datasets. Furthermore, due to the small scale of the target dataset, we construct a feature cloud for each target sample and leverage contrastive learning to extract more discriminative features. Lastly, we propose adaptive feature cloud expansion to push the decision boundary to a low-density area. Unlike most existing transfer learning methods that aim only to mitigate the domain gap, our method instead simultaneously considers the domain gap and the data deficiency problem of the target dataset. The conditional domain adaptation and the feature cloud generation of our method are learning jointly to extract discriminative features in an end-to-end manner. Besides, the adaptive feature cloud expansion improves the modelâ€™s generalisation ability in the target domain. Extensive experiments on pneumonia and COVID-19 diagnosis tasks demonstrate that our method outperforms several state-of-the-art unsupervised domain adaptation approaches, which verifies the effectiveness of CDACM for automated pneumonia diagnosis using chest X-ray imaging.},
  pdf={MedIA2022_CDACM.pdf},
  preview={CDACM_2022.jpeg}
}

@article{wang2022adversarial,
  title={Adversarial multimodal fusion with attention mechanism for skin lesion classification using clinical and dermoscopic images},
  author={Wang, Yan and Feng, Yangqin and Zhang, Lei and Zhou, Joey Tianyi and Liu, Yong and Goh, Rick Siow Mong and Zhen*, Liangli},
  journal={Medical Image Analysis},
  volume={81},
  pages={102535},
  year={2022},
  publisher={Elsevier},
  abbr={MedIA},
  pdf={MedIA2022_AMFAM.pdf},
  preview={MedIA_2022.png},
  abstract = {Accurate skin lesion diagnosis requires a great effort from experts to identify the characteristics from clinical and dermoscopic images. Deep multimodal learning-based methods can reduce intra- and inter-reader variability and improve diagnostic accuracy compared to the single modality-based methods. This study develops a novel method, named adversarial multimodal fusion with attention mechanism (AMFAM), to perform multimodal skin lesion classification. Specifically, we adopt a discriminator that uses adversarial learning to enforce the feature extractor to learn the correlated information explicitly. Moreover, we design an attention-based reconstruction strategy to encourage the feature extractor to concentrate on learning the features of the lesion area, thus, enhancing the feature vector from each modality with more discriminative information. Unlike existing multimodal-based approaches, which only focus on learning complementary features from dermoscopic and clinical images, our method considers both correlated and complementary information of the two modalities for multimodal fusion. To verify the effectiveness of our method, we conduct comprehensive experiments on a publicly available multimodal and multi-task skin lesion classification dataset: 7-point criteria evaluation database. The experimental results demonstrate that our proposed method outperforms the current state-of-the-art methods and improves the average AUC score by above 2% on the test set.}
}


@article{zhen2022deep,
  title={Deep multimodal transfer learning for cross-modal retrieval},
  author={Zhen, Liangli and Hu, Peng and Peng, Xi and Goh, Rick Siow Mong and Zhou, Joey Tianyi},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  abbr={IEEE TNNLS},
  volume={33},
  number={2},
  pages={798--810},
  year={2022},
  pdf={TNNLS2022_DMTL_CMR.pdf},
  preview={TNNLS2022_MTL.png},
  abstract={Cross-modal retrieval (CMR) enables flexible retrieval experience across different modalities (e.g., texts versus images), which maximally benefits us from the abundance of multimedia data. Existing deep CMR approaches commonly require a large amount of labeled data for training to achieve high performance. However, it is time-consuming and expensive to annotate the multimedia data manually. Thus, how to transfer valuable knowledge from existing annotated data to new data, especially from the known categories to new categories, becomes attractive for real-world applications. To achieve this end, we propose a deep multimodal transfer learning (DMTL) approach to transfer the knowledge from the previously labeled categories (source domain) to improve the retrieval performance on the unlabeled new categories (target domain). Specifically, we employ a joint learning paradigm to transfer knowledge by assigning a pseudolabel to each target sample. During training, the pseudolabel is iteratively updated and passed through our model in a self-supervised manner. At the same time, to reduce the domain discrepancy of different modalities, we construct multiple modality-specific neural networks to learn a shared semantic space for different modalities by enforcing the compactness of homoinstance samples and the scatters of heteroinstance samples. Our method is remarkably different from most of the existing transfer learning approaches. To be specific, previous works usually assume that the source domain and the target domain have the same label set. In contrast, our method considers a more challenging multimodal learning situation where the label sets of the two domains are different or even disjoint. Experimental studies on four widely used benchmarks validate the effectiveness of the proposed method in multimodal transfer learning and demonstrate its superior performance in CMR compared with 11 state-of-the-art methods.}
}

@article{zhang2022aug,
  title={Augmented multi-party computation against gradient leakage in federated learning},
  author={Zhang, Chi and Sotthiwat, Ekanut and Zhen*, Liangli and Li, Zengxiang},
  journal={IEEE Transactions on Big Data},
  abbr={IEEE TBD},
  year={2022},
  preview={TBD22_AMPC.png},
  pdf={TBD22_AMPC.pdf},
  abstract={Multi-Party Computation (MPC) provides an effective cryptographic solution for distributed computing systems so that local models with sensitive information are encrypted before sending to the centralized servers for aggregation. Though direct local knowledge leakages are eliminated in MPC-based algorithms, we observe the server can still obtain the local information indirectly in many scenarios, or even reveal the groundtruth images through methods like Deep Leakage from Gradients (DLG). To eliminate such possibilities and provide stronger protections, we propose an augmented MPC approach by encrypting local models with two rounds of decomposition before transmitting to the server. The proposed solution allows us to remove the constraint that servers must be honest in the general federated learning settings since the true global model is hidden from the servers. Specifically,  the augmented MPC algorithm encodes local models into multiple secret shares in the first round, then each share is furthermore split into a public share and a private share. The consequences of such a two-round decomposition are that the augmented algorithm fully inherits the advantages of standard MPC by providing lossless encryption and decryption while simultaneously rendering the global model invisible to the central server. Both theoretical analysis and experimental verification demonstrate that such an augmented solution can provide stronger protections for the security and privacy of the training data, with minimal extra communication and computation costs incurred.}
}

@inproceedings{du2022efficient,
  title={Efficient sharpness-aware minimization for improved training of neural networks},
  author={Du, Jiawei and Yan, Hanshu and Feng, Jiashi and Zhou, Joey Tianyi and Zhen, Liangli and Goh, Rick Siow Mong and Tan, Vincent YF},
  booktitle={ICLR},
  abbr={ICLR-2022},
  year={2022},
  pdf={ICLR2022_efficient_sharpness_aware_mini.pdf},
  code={https://github.com/dydjw9/efficient_sam},
  preview={ICLR22_ESAM.png},
  abstract={Overparametrized Deep Neural Networks (DNNs) often achieve astounding performances, but may potentially result in severe generalization error. Recently, the relation between the sharpness of the loss landscape and the generalization error has been established by Foret et al. (2020), in which the Sharpness Aware Minimizer (SAM) was proposed to mitigate the degradation of the generalization. Unfortunately, SAMâ€™s computational cost is roughly double that of base optimizers, such as Stochastic Gradient Descent (SGD). This paper thus proposes Efficient Sharpness Aware Minimizer (ESAM), which boosts SAMâ€™s efficiency at no cost to its generalization performance. ESAM includes two novel and efficient training strategiesâ€”StochasticWeight Perturbation and Sharpness-Sensitive Data Selection. In the former, the sharpness measure is approximated by perturbing a stochastically chosen set of weights in each iteration; in the latter, the SAM loss is optimized using only a judiciously selected subset of data that is sensitive to the sharpness. We provide theoretical explanations as to why these strategies perform well. We also show, via extensive experiments on the CIFAR and ImageNet datasets, that ESAM enhances the efficiency over SAM from requiring 100% extra computations to 40% vis-`a-vis base optimizers, while test accuracies are preserved or even improved.}
}

@article{feng2022deep,
  title={Deep supervised domain adaptation for pneumonia diagnosis from chest X-ray images},
  author={Feng, Yangqin and Xu, Xinxing and Wang, Yan and Lei, Xiaofeng and Teo, Soo Kng and Ting, Jordan Sim Zheng and Ting, Yonghan and Zhen, Liangli and Zhou, Joey Tianyi and Liu, Yong and others},
  journal={IEEE Journal of Biomedical and Health Informatics},
  abbr={IEEE JBHI},
  volume={26},
  number={3},
  pages={1080--1090},
  year={2022},
  pdf={JBHI2021-Deep_Supervised_Domain_Adaptation_for_Pneumonia_Diagnosis_From_Chest_X-Ray_Images.pdf},
  publisher={IEEE},
  preview={JBHI22_DSDA.png},
  abstract={Pneumonia is one of the most common treatable causes of death, and early diagnosis allows for early intervention. Automated diagnosis of pneumonia can therefore improve outcomes. However, it is challenging to develop high-performance deep learning models due to the lack of well-annotated data for training. This paper proposes a novel method, called Deep Supervised Domain Adaptation (DSDA), to automatically diagnose pneumonia from chest X-ray images. Specifically, we propose to transfer the knowledge from a publicly available large-scale source dataset (ChestX-ray14) to a well-annotated but small-scale target dataset (the TTSH dataset). DSDA aligns the distributions of the source domain and the target domain according to the underlying semantics of the training samples. It includes two task-specific sub-networks for the source domain and the target domain, respectively. These two sub-networks share the feature extraction layers and are trained in an end-to-end manner. Unlike most existing domain adaptation approaches that perform the same tasks in the source domain and the target domain, we attempt to transfer the knowledge from a multi-label classification task in the source domain to a binary classification task in the target domain. To evaluate the effectiveness of our method, we compare it with several existing peer methods. The experimental results show that our method can achieve promising performance for automated pneumonia diagnosis.}
}

@article{hu2022deep,
  title={Deep semi-supervised multi-view learning with increasing views},
  author={Hu, Peng and Peng, Xi and Zhu, Hongyuan and Zhen, Liangli and Lin, Jie and Yan, Huaibai and Peng, Dezhong},
  journal={IEEE Transactions on Cybernetics},
  abbr={IEEE TCYB},
  volume={52},
  number={12},
  pages={12954--12965},
  year={2022},
  preview={tcyb21_deepse.png},
  abstract={In this article, we study two challenging problems in semisupervised cross-view learning. On the one hand, most existing methods assume that the samples in all views have a pairwise relationship, that is, it is necessary to capture or establish the correspondence of different views at the sample level. Such an assumption is easily isolated even in the semisupervised setting wherein only a few samples have labels that could be used to establish the correspondence. On the other hand, almost all existing multiview methods, including semisupervised ones, usually train a model using a fixed dataset, which cannot handle the data of increasing views. In practice, the view number will increase when new sensors are deployed. To address the above two challenges, we propose a novel method that employs multiple independent semisupervised view-specific networks (ISVNs) to learn representation for multiple views in a view-decoupling fashion. The advantages of our method are two-fold. Thanks to our specifically designed autoencoder and pseudolabel learning paradigm, our method shows an effective way to utilize both the labeled and unlabeled data while relaxing the data assumption of the pairwise relationship, that is, correspondence. Furthermore, with our view decoupling strategy, the proposed ISVNs could be separately trained, thus efficiently handling the data of increasing views without retraining the entire model. To the best of our knowledge, our ISVN could be one of the first attempts to make handling increasing views in the semisupervised setting possible, as well as an effective solution to the noncorresponding problem. To verify the effectiveness and efficiency of our method, we conduct comprehensive experiments by comparing 13 state-of-the-art approaches on four multiview datasets in terms of retrieval and classification.},
  pdf={TCYB2021_Deep_Semisupervised_Multiview_Learning_With_Increasing_Views.pdf}
}

@article{zhang2022natural,
  title={Natural language video localization: A revisit in span-based question answering framework},
  author={Zhang, Hao and Sun, Aixin and Jing, Wei and Zhen, Liangli and Zhou, Joey Tianyi and Goh, Rick Siow Mong},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  abbr={IEEE TPAMI},
  volume={44},
  number={8},
  pages={4252--4266},
  year={2022},
  publisher={IEEE Computer Society},
  code={https://github.com/IsaacChanghau/VSLNet},
  pdf={TPAMI2022_Natural_Language_Video_Localization_A_Revisit_in_Span-Based_Question_Answering_Framework.pdf},
  preview={TPAMI22_NLVL.png},
  abstract={Natural Language Video Localization (NLVL) aims to locate a target moment from an untrimmed video that semantically corresponds to a text query. Existing approaches mainly solve the NLVL problem from the perspective of computer vision by formulating it as ranking, anchor, or regression tasks. These methods suffer from large performance degradation when localizing on long videos. In this work, we address the NLVL from a new perspective, i.e., span-based question answering (QA), by treating the input video as a text passage. We propose a video span localizing network (VSLNet), on top of the standard span-based QA framework (named VSLBase), to address NLVL. VSLNet tackles the differences between NLVL and span-based QA through a simple yet effective query-guided highlighting (QGH) strategy. QGH guides VSLNet to search for the matching video span within a highlighted region. To address the performance degradation on long videos, we further extend VSLNet to VSLNet-L by applying a multi-scale split-and-concatenation strategy. VSLNet-L first splits the untrimmed video into short clip segments; then, it predicts which clip segment contains the target moment and suppresses the importance of other segments. Finally, the clip segments are concatenated, with different confidences, to locate the target moment accurately. Extensive experiments on three benchmark datasets show that the proposed VSLNet and VSLNet-L outperform the state-of-the-art methods; VSLNet-L addresses the issue of performance degradation on long videos. Our study suggests that the span-based QA framework is an effective strategy to solve the NLVL problem.}
}


@article{wang2021evolutionary,
  title={Evolutionary multi-objective model compression for deep neural networks},
  author={Wang, Zhehui and Luo, Tao and Li, Miqing and Zhou, Joey Tianyi and Goh, Rick Siow Mong and Zhen*, Liangli},
  journal={IEEE Computational Intelligence Magazine},
  abbr={IEEE CIM},
  volume={16},
  number={3},
  pages={10--21},
  year={2021},
  publisher={IEEE},
  abstract={While deep neural networks (DNNs) deliver state-of-the-art accuracy on various applications from face recognition to language translation, it comes at the cost of high computational and space complexity, hindering their deployment on edge devices. To enable efficient processing of DNNs in inference, a novel approach, called Evolutionary Multi-Objective Model Compression (EMOMC), is proposed to optimize energy efficiency (or model size) and accuracy simultaneously. Specifically, the network pruning and quantization space are explored and exploited by using architecture population evolution. Furthermore, by taking advantage of the orthogonality between pruning and quantization, a two-stage pruning and quantization co-optimization strategy is developed, which considerably reduces time cost of the architecture search. Lastly, different dataflow designs and parameter coding schemes are considered in the optimization process since they have a significant impact on energy consumption and the model size. Owing to the cooperation of the evolution between different architectures in the population, a set of compact DNNs that offer trade-offs on different objectives (e.g., accuracy, energy efficiency and model size) can be obtained in a single run. Unlike most existing approaches designed to reduce the size of weight parameters with no significant loss of accuracy, the proposed method aims to achieve a trade-off between desirable objectives, for meeting different requirements of various edge devices. Experimental results demonstrate that the proposed approach can obtain a diverse population of compact DNNs that are suitable for a broad range of different memory usage and energy consumption requirements. Under negligible accuracy loss, EMOMC improves the energy efficiency and model compression rate of VGG-16 on CIFAR-10 by a factor of more than 8.9 X and 2.4 X, respectively.},
  pdf={CIM2021-Evolutionary_Multi-Objective_Model_Compression_for_Deep_Neural_Networks.pdf},
  code={https://github.com/liangli-zhen/EMOMC},
  preview={CIM21_EMOMC.png}
}

@inproceedings{hu2021learning,
  title={Learning cross-modal retrieval with noisy labels},
  author={Hu, Peng and Peng, Xi and Zhu, Hongyuan and Zhen, Liangli and Lin, Jie},
  booktitle={CVPR},
  abbr={CVPR-2021},
  pages={5403--5413},
  year={2021},
  preview={CVPR21_Learning.png},
  pdf={https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Learning_Cross-Modal_Retrieval_With_Noisy_Labels_CVPR_2021_paper.pdf},
  abstract={Recently, cross-modal retrieval is emerging with the help of deep multimodal learning. However, even for unimodal data, collecting large-scale well-annotated data is expensive and time-consuming, and not to mention the additional challenges from multiple modalities. Although crowd-sourcing annotation, e.g., Amazonâ€™s Mechanical Turk, can be utilized to mitigate the labeling cost, but leading to the unavoidable noise in labels for the non-expert annotating. To tackle the challenge, this paper presents a general Multi-modal Robust Learning framework (MRL) for learning with multimodal noisy labels to mitigate noisy samples and correlate distinct modalities simultaneously. To be specific, we propose a Robust Clustering loss (RC) to make the deep networks focus on clean samples instead of noisy ones. Besides, a simple yet effective multimodal loss function, called Multimodal Contrastive loss (MC), is proposed to maxi-mize the mutual information between different modalities, thus alleviating the interference of noisy samples and cross-modal discrepancy. Extensive experiments are conducted on four widely-used multimodal datasets to demonstrate the effectiveness of the proposed approach by comparing to 14 state-of-the-art methods.},
  pdf={CVPR2021_Learning_Cross-Modal_Retrieval_With_Noisy_Labels_CVPR_2021_paper.pdf}
}

@inproceedings{zhang2021video,
  title={Video corpus moment retrieval with contrastive learning},
  author={Zhang, Hao and Sun, Aixin and Jing, Wei and Nan, Guoshun and Zhen, Liangli and Zhou, Joey Tianyi and Goh, Rick Siow Mong},
  booktitle={SIGIR},
  abbr={SIGIR-2021},
  year={2021},
  abstract={Given a collection of untrimmed and unsegmented videos, video corpus moment retrieval (VCMR) is to retrieve a temporal moment (i.e., a fraction of a video) that semantically corresponds to a given text query. As video and text are from two distinct feature spaces, there are two general approaches to address VCMR: (i) to separately encode each modality representations, then align the two modality representations for query processing, and (ii) to adopt fine-grained cross-modal interaction to learn multi-modal representations for query processing. While the second approach often leads to better retrieval accuracy, the first approach is far more efficient. In this paper, we propose a Retrieval and Localization Network with Contrastive Learning (ReLoCLNet) for VCMR. We adopt the first approach and introduce two contrastive learning objectives to refine video encoder and text encoder to learn video and text representations separately but with better alignment for VCMR. The video contrastive learning (VideoCL) is to maximize mutual information between query and candidate video at video-level. The frame contrastive learning (FrameCL) aims to highlight the moment region corresponds to the query at frame-level, within a video. Experimental results show that, although ReLoCLNet encodes text and video separately for efficiency, its retrieval accuracy is comparable with baselines adopting cross-modal interaction learning.},
  pdf={SIGIR2021_Video_corpus_moment_retrieval.pdf},
  code={https://github.com/IsaacChanghau/ReLoCLNet},
  preview={SIGIR2021_Video.png}
}

@article{hu2021cross,
  title={Cross-modal discriminant adversarial network},
  author={Hu, Peng and Peng, Xi and Zhu, Hongyuan and Lin, Jie and Zhen, Liangli and Wang, Wei and Peng, Dezhong},
  journal={Pattern Recognition},
  abbr={PR},
  volume={112},
  pages={107734},
  year={2021},
  publisher={Elsevier},
  preview={PR21_HU.png},
  abstract={Cross-modal retrieval aims at retrieving relevant points across different modalities, such as retrieving images via texts. One key challenge of cross-modal retrieval is narrowing the heterogeneous gap across diverse modalities. To overcome this challenge, we propose a novel method termed as Cross-modal discriminant Adversarial Network (CAN). Taking bi-modal data as a showcase, CAN consists of two parallel modality-specific generators, two modality-specific discriminators, and a Cross-modal Discriminant Mechanism (CDM). To be specific, the generators project diverse modalities into a latent cross-modal discriminant space. Meanwhile, the discriminators compete against the generators to alleviate the heterogeneous discrepancy in this space, i.e., the generators try to generate unified features to confuse the discriminators, and the discriminators aim to classify the generated results. To further remove the redundancy and preserve the discrimination, we propose CDM to project the generated results into a single common space, accompanying with a novel eigenvalue-based loss. Thanks to the eigenvalue-based loss, CDM could push as much discriminative power as possible into all latent directions. To demonstrate the effectiveness of our CAN, comprehensive experiments are conducted on four multimedia datasets comparing with 15 state-of-the-art approaches.},
  pdf={PR2020_Cross-modal Discriminant Adversarial Network.pdf}
}

@article{hu2021automated,
  title={Automated building extraction using satellite remote sensing imagery},
  author={Hu, Qintao and Zhen, Liangli and Mao, Yao and Zhou, Xi and Zhou, Guozhong},
  journal={Automation in Construction},
  abbr={Autom},
  volume={123},
  pages={103509},
  year={2021},
  preview={AC20_auto.png},
  pdf={AC2021-Automated building extraction using satellite remote sensing imagery.pdf},
  abstract={Automatic extraction of buildings from remote sensing images plays a critical role in urban planning and digital city construction applications. In real-world applications, however, real scenes can be highly complex (e.g., various building structures and shapes, presence of obstacles, and low contrast between buildings and surrounding regions), making automatic building extraction extremely challenging. To conquer this challenge, we propose a novel method called Deep Automatic Building Extraction Network (DABE-Net). It adopts squeeze-and-excitation (SE) operations and the residual recurrent convolutional neural network (RRCNN) to construct building-blocks. Furthermore, an attention mechanism is introduced into the network to improve segmentation accuracy. Specifically, to handle small buildings, we highlight small buildings and develop a multi-scale segmentation loss function. The theoretical analysis and experimental results show that the proposed method is effective in building extraction and outperforms several peer methods on the dataset of Mapping challenge competition.},
  publisher={Elsevier}
}

@article{wang2021drsl,
  title={DRSL: Deep relational similarity learning for cross-modal retrieval},
  author={Wang, Xu and Hu, Peng and Zhen, Liangli and Peng, Dezhong},
  journal={Information Sciences},
  abbr={INS},
  volume={546},
  pages={298--311},
  year={2021},
  publisher={Elsevier},
  pdf={INS2021_DRSL.pdf},
  preview={INS20_DRSL.png},
  abstract={Cross-modal retrieval aims to retrieve relevant samples across different media modalities. Existing cross-modal retrieval approaches are contingent on learning common representations of all modalities by assuming that an equal amount of information exists in different modalities. However, since the quantity of information among cross-modal samples is unbalanced and unequal, it is inappropriate to directly match the obtained modality-specific representations across different modalities in a common space. In this paper, we propose a new method called Deep Relational Similarity Learning (DRSL) for cross-modal retrieval. Unlike existing approaches, the proposed DRSL aims to effectively bridge the heterogeneity gap of different modalities by directly learning the natural pairwise similarities instead of explicitly learning a common space. DRSL is a deep hybrid framework that integrates the relation networks module for relation learning, capturing the implicit nonlinear distance metric. To the best of our knowledge, DRSL is the first approach that incorporates relation networks into the cross-modal learning scenario. Comprehensive experimental results show that the proposed DRSL model achieves state-of-the-art results in cross-modal retrieval tasks on four widely-used benchmark datasets, i.e., Wikipedia, Pascal Sentences, NUS-WIDE-10K, and XMediaNet.},
  code={https://github.com/wangxu-scu/DRSL}
}

@article{hu2021joint,
  title={Joint versus independent multi-view hashing for cross-view retrieval},
  author={Hu, Peng and Peng, Xi and Zhu, Hongyuan and Lin, Jie and Zhen, Liangli and Peng, Dezhong},
  journal={IEEE Transactions on Cybernetics},
  abbr={IEEE TCYB},
  volume={51},
  number={10},
  pages={4982--4993},
  year={2021},
  preview={tcyb21_joint.png},
  abstract={Thanks to the low storage cost and high query speed, cross-view hashing (CVH) has been successfully used for similarity search in multimedia retrieval. However, most existing CVH methods use all views to learn a common Hamming space, thus making it difficult to handle the data with increasing views or a large number of views. To overcome these difficulties, we propose a decoupled CVH network (DCHN) approach which consists of a semantic hashing autoencoder module (SHAM) and multiple multiview hashing networks (MHNs). To be specific, SHAM adopts a hashing encoder and decoder to learn a discriminative Hamming space using either a few labels or the number of classes, that is, the so-called flexible inputs. After that, MHN independently projects all samples into the discriminative Hamming space that is treated as an alternative ground truth. In brief, the Hamming space is learned from the semantic space induced from the flexible inputs, which is further used to guide view-specific hashing in an independent fashion. Thanks to such an independent/decoupled paradigm, our method could enjoy high computational efficiency and the capacity of handling the increasing number of views by only using a few labels or the number of classes. For a newly coming view, we only need to add a view-specific network into our model and avoid retraining the entire model using the new and previous views. Extensive experiments are carried out on five widely used multiview databases compared with 15 state-of-the-art approaches. The results show that the proposed independent hashing paradigm is superior to the common joint ones while enjoying high efficiency and the capacity of handling newly coming views.},
  pdf={TCYB2020_Joint Versus Independent Multiview Hashing for Cross-View Retrieval.pdf}
}


@inproceedings{sotthiwat2021partially,
  title={Partially encrypted multi-party computation for federated learning},
  author={Sotthiwat, Ekanut and Zhen*, Liangli and Li, Zengxiang and Zhang*, Chi},
  booktitle={CCGrid},
  abbr={CCGrid-2021},
  preview={ccgrid21_partially.png},
  year={2021},
  abstract={Multi-party computation (MPC) allows distributed machine learning to be performed in a privacy-preserving manner so that end-hosts are unaware of the true models on the clients. However, the standard MPC algorithm also triggers additional communication and computation costs, due to those expensive cryptography operations and protocols. In this paper, instead of applying heavy MPC over the entire local models for secure model aggregation, we propose to encrypt critical part of model (gradients) parameters to reduce communication cost, while maintaining MPCâ€™s advantages on privacy-preserving without sacrificing accuracy of the learnt joint model. Theoretical analysis and experimental results are provided to verify that our proposed method could prevent deep leakage from gradients attacks from reconstructing original data of individual participants. Experiments using deep learning models over the MNIST and CIFAR-10 datasets empirically demonstrate that our proposed partially encrypted MPC method can reduce the communication and computation cost significantly when compared with conventional MPC, and it achieves as high accuracy as traditional distributed learning which aggregates local models using plain text.},
  pdf={CCGrid2021-Partially_Encrypted_Multi-Party_Computation_for_Federated_Learning.pdf}
}

@inproceedings{zhang2021parallel,
  title={Parallel attention network with sequence matching for video grounding},
  author={Zhang, Hao and Sun, Aixin and Jing, Wei and Zhen, Liangli and Zhou, Joey Tianyi and Goh, Rick Siow Mong},
  booktitle={Findings of ACL},
  abbr={ACL-2021},
  preview={acl21_parallel.png},
  code={https://github.com/IsaacChanghau/SeqPAN},
  pdf={https://aclanthology.org/2021.findings-acl.69.pdf},
  abstract={Given a video, video grounding aims to retrieve a temporal moment that semantically corresponds to a language query. In this work, we propose a Parallel Attention Network with Sequence matching (SeqPAN) to address the challenges in this task: multi-modal representation learning, and target moment boundary prediction. We design a self-guided parallel attention module to effectively capture self-modal contexts and cross-modal attentive information between video and text. Inspired by sequence labeling tasks in natural language processing, we split the ground truth moment into begin, inside, and end regions. We then propose a sequence matching strategy to guide start/end boundary predictions using region labels. Experimental results on three datasets show that SeqPAN is superior to state-of-the-art methods. Furthermore, the effectiveness of the self-guided parallel attention module and the sequence matching module is verified.},
  year={2021}
}


@article{liu2021automated,
  title={Automated deepfake detection},
  author={Liu, Ping and Lin, Yuewei and He, Yang and Wei, Yunchao and Zhen, Liangli and Zhou, Joey Tianyi and Goh, Rick Siow Mong and Liu, Jingen},
  journal={arXiv preprint arXiv:2106.10705},
  year={2021},
  abstract={In this paper, we propose to utilize Automated Machine Learning to adaptively search a neural architecture for deepfake detection. This is the first time to employ automated machine learning for deepfake detection. Based on our explored search space, our proposed method achieves competitive prediction accuracy compared to previous methods. To improve the generalizability of our method, especially when training data and testing data are manipulated by different methods, we propose a simple yet effective strategy in our network learning process: making it to estimate potential manipulation regions besides predicting the real/fake labels. Unlike previous works manually design neural networks, our method can relieve us from the high labor cost in network construction. More than that, compared to previous works, our method depends much less on prior knowledge, e.g., which manipulation method is utilized or where exactly the fake image is manipulated. Extensive experimental results on two benchmark datasets demonstrate the effectiveness of our proposed method for deepfake detection.},
  abbr={arXiv},
  preview={arxiv2021_AFD.png},
  pdf={arxiv2021_AFD.pdf}
}

@article{zhang2021distributed,
  title={Distributed monitoring for energy infrastructures: A two-tier analysis over wireless networks},
  author={Zhang, Chi and Zhen, Liangli and Zhou, Joey Tianyi and Chen, Cen},
  journal={IEEE Wireless Communication},
  abbr={IEEE WCM},
  volume={28},
  number={6},
  pages={13--18},
  year={2021},
  preview={wcm21_dist.png},
  abstract={Wireless networks (e.g., 5G networks) enable distributed energy infrastructures to be connected even when they are geometrically isolated. Intelligent monitoring from remote sites therefore becomes possible, allowing decision makers to examine the status of distributed energy infrastructures from a central location. The major challenge is when local devices cannot perform the monitoring independently; transmitting every signal back to the central server triggers enormous amounts of wireless communication. To address this, we propose a two-tier AI system by offloading computations to multiple devices. Specifically, we build lightweight AI models for deployment on edge clients (i.e., edge sensors) and a large-scale AI model for the central server. These two types of AI models are trained with different criteria: the models on the edges act as the filtering tools to detect abnormal events and maximally avoid making false negative predictions, whereas the server model is supposed to be an expert for accurate predictions. By validating on a power theft dataset, we show that such a cascading methodology could filter out sufficient negative examples on the edge side while still being able to provide precise predictions on the second-round analysis.},
  pdf={WCM2021_Distributed_Monitoring_for_Energy_Infrastructures_A_Two-Tier_Analysis_over_Wireless_Networks.pdf}
}

@article{zhen2020kernel,
  title={Kernel truncated regression representation for robust subspace clustering},
  author={Zhen, Liangli and Peng, Dezhong and Wang, Wei and Yao, Xin},
  journal={Information Sciences},
  abbr={INS},
  volume={524},
  pages={59--76},
  year={2020},
  publisher={Elsevier},
  preview={ins20_kernel.png},
  abstract={Subspace clustering aims to group data points into multiple clusters of which each corresponds to one subspace. Most existing subspace clustering approaches assume that input data lie on linear subspaces. In practice, however, this assumption usually does not hold. To achieve nonlinear subspace clustering, we propose a novel method, called kernel truncated regression representation. Our method consists of the following four steps: 1) projecting the input data into a hidden space, where each data point can be linearly represented by other data points; 2) calculating the linear representation coefficients of the data representations in the hidden space; 3) truncating the trivial coefficients to achieve robustness and block-diagonality; and 4) executing the graph cutting operation on the coefficient matrix by solving a graph Laplacian problem. Our method has the advantages of a closed-form solution and the capacity of clustering data points that lie on nonlinear subspaces. The first advantage makes our method efficient in handling large-scale datasets, and the second one enables the proposed method to conquer the nonlinear subspace clustering challenge. Extensive experiments on six benchmarks demonstrate the effectiveness and the efficiency of the proposed method in comparison with current state-of-the-art approaches.},
  pdf={INS2020_Kernel Truncated Regression Representation for Robust Subspace Clustering.pdf},
  code={https://liangli-zhen.github.io/assets/code/KTRR.zip}
}

@article{zhen2020objective,
  title={Objective reduction for visualising many-objective solution sets},
  author={Zhen, Liangli and Li, Miqing and Peng, Dezhong and Yao, Xin},
  journal={Information Sciences},
  abbr={INS},
  volume={512},
  pages={278--294},
  year={2020},
  preview={ins20_objective.png},
  abstract={Visualising a solution set is of high importance in many-objective optimisation. It can help algorithm designers understand the performance of search algorithms and decision makers select their preferred solution(s). In this paper, an objective reduction-based visualisation method (ORV) is proposed to view many-objective solution sets. ORV attempts to map a solution set from a high-dimensional objective space into a low-dimensional space while preserving the distribution and the Pareto dominance relation between solutions in the set. Specifically, ORV sequentially decomposes objective vectors which can be linearly represented by their positively correlated objective vectors until the expected number of preserved objective vectors is reached. ORV formulates the objective reduction as a solvable convex problem. Extensive experiments on both synthetic and real-world problems have verified the effectiveness of the proposed method.},
  pdf={INS2020_Objective_reduction_for_visualising_many-objective_solution_sets.pdf},
  code={https://liangli-zhen.github.io/assets/code/ORV_MATLAB.zip}
}

@article{zhen2020underdetermined,
  title={Underdetermined mixing matrix estimation by exploiting sparsity of sources},
  author={Zhen, Liangli and Peng, Dezhong and Zhang, Haixian and Sang, Yongsheng and Zhang, Lijun},
  journal={Measurement},
  abbr={Measurement},
  volume={152},
  pages={107268},
  year={2020},
  publisher={Elsevier},
  preview={measurement20_under.png},
  abstract={To estimate the mixing matrix in underdetermined mixing systems, we propose a novel method by exploiting the sparsity of sources. We utilize the pairwise relationships among all of the mixture representations to detect the single source points in the time-frequency (TF) domain, i.e., the positions where only one source contributed dominantly. The mixture representations at these single source points are then clustered to estimate the underlying mixing matrix. Since the pairwise relationships among all mixtures are considered in the TF domain, the proposed method can achieve an accurate mixing matrix estimation and be robust in noisy cases. Experimental results indicate that our method is effective in mixing matrix estimation and outperforms five peer methods.},
  pdf={Measurement2020_Underdetermined_mixing_matrix_estimation_by_exploiting_sparsity_of_sources.pdf}
}

@article{hu2020adaptive,
  title={An adaptive stochastic parallel gradient descent approach for efficient fiber coupling},
  author={Hu, Qintao and Zhen, Liangli and Mao, Yao and Zhu, Shiwei and Zhou, Xi and Zhou, Guozhong},
  journal={Optics Express},
  abbr={Optics Express},
  volume={28},
  number={9},
  pages={13141--13154},
  year={2020},
  publisher={The Optical Society},
  preview={OE2020_Hu.png},
  abstract={In high-speed free-space optical communication systems, the received laser beam must be coupled into a single-mode fiber at the input of the receiver module. However, propagation through atmospheric turbulence degrades the spatial coherence of a laser beam and poses challenges for fiber coupling. In this paper, we propose a novel method, called as adaptive stochastic parallel gradient descent (ASPGD), to achieve efficient fiber coupling. To be specific, we formulate the fiber coupling problem as a model-free optimization problem and solve it using ASPGD in parallel. To avoid converging to the extremum points and accelerate its convergence speed, we integrate the momentum and the adaptive gain coefficient estimation to the original stochastic parallel gradient descent (SPGD) method. Simulation and experimental results demonstrate that the proposed method reduces 50% of iterations, while keeping the stability by comparing it with the original SPGD method.},
  pdf={OE_Adaptive stochastic parallel gradient descent.pdf}
}

@inproceedings{zhen2019deep,
  title={Deep supervised cross-modal retrieval},
  author={Zhen, Liangli and Hu, Peng and Wang, Xu and Peng, Dezhong},
  booktitle={CVPR},
  abbr={CVPR-2019},
  year={2019},
  pdf={CVPR2019_Deep_Supervised_Cross_modal_Retrieval.pdf},
  preview={CVPR19_deep.png},
  abstract={Cross-modal retrieval aims to enable flexible retrieval across different modalities. The core of cross-modal retrieval is how to measure the content similarity between different types of data. In this paper, we present a novel cross-modal retrieval method, called Deep Supervised Cross-modal Retrieval (DSCMR). It aims to find a common representation space, in which the samples from different modalities can be compared directly. Specifically, DSCMR minimises the discrimination loss in both the label space and the common representation space to supervise the model learning discriminative features. Furthermore, it simultaneously minimises the modality invariance loss and uses a weight sharing strategy to eliminate the cross-modal discrepancy of multimedia data in the common representation space to learn modality-invariant features. Comprehensive experimental results on four widely-used benchmark datasets demonstrate that the proposed method is effective in cross-modal learning and significantly outperforms the state-of-the-art cross-modal retrieval methods.},
  code={https://liangli-zhen.github.io/assets/code/DSCMR.zip}
}

@inproceedings{hu2019scalable,
  title={Scalable deep multimodal learning for cross-modal retrieval},
  author={Hu, Peng and Zhen, Liangli and Peng, Dezhong and Liu, Pei},
  booktitle={SIGIR},
  abbr = {SIGIR-2019},
  year={2019},
  preview={sigir19_scalable.png},
  pdf={SIGIR2019_Scalable Deep Multimodal Learning for Cross-Modal Retrieval.pdf},
  abstract={Cross-modal retrieval takes one type of data as the query to retrieve relevant data of another type. Most of existing cross-modal retrieval approaches were proposed to learn a common subspace in a joint manner, where the data from all modalities have to be involved during the whole training process. For these approaches, the optimal parameters of different modality-specific transformations are dependent on each other and the whole model has to be retrained when handling samples from new modalities. In this paper, we present a novel cross-modal retrieval method, called Scalable Deep Multimodal Learning (SDML). It proposes to predefine a common subspace, in which the between-class variation is maximized while the within-class variation is minimized. Then, it trains m modality-specific networks for m modalities (one network for each modality) to transform the multimodal data into the predefined common subspace to achieve multimodal learning. Unlike many of the existing methods, our method can train different modality-specific networks independently and thus be scalable to the number of modalities. To the best of our knowledge, the proposed SDML could be one of the first works to independently project data of an unfixed number of modalities into a predefined common subspace. Comprehensive experimental results on four widely-used benchmark datasets demonstrate that the proposed method is effective and efficient in multimodal learning and outperforms the state-of-the-art methods in cross-modal retrieval.},
}

@inproceedings{hu2019separated,
  title={Separated variational hashing networks for cross-modal retrieval},
  author={Hu, Peng and Wang, Xu and Zhen, Liangli and Peng, Dezhong},
  booktitle={ACM-MM},
  abbr={ACM MM},
  pages={1721--1729},
  year={2019},
  pdf={MM2019_Separated_variational_hashing_networks_for_cross-modal_retrieval.pdf},
  preview={mm19_sep.png},
  abstract={Cross-modal hashing, due to its low storage cost and high query speed, has been successfully used for similarity search in multimedia retrieval applications. It projects high-dimensional data into a shared isomorphic Hamming space with similar binary codes for semantically-similar data. In some applications, all modalities may not be obtained or trained simultaneously for some reasons, such as privacy, secret, storage limitation, and computational resource limitation. However, most existing cross-modal hashing methods need all modalities to jointly learn the common Hamming space, thus hindering them from handling these problems. In this paper, we propose a novel approach called Separated Variational Hashing Networks (SVHNs) to overcome the above challenge. Firstly, it adopts a label network (LabNet) to exploit available and nonspecific label annotations to learn a latent common Hamming space by projecting each semantic label into a common binary representation. Then, each modality-specific network can separately map the samples of the corresponding modality into their binary semantic codes learned by LabNet. We achieve it by conducting variational inference to match the aggregated posterior of the hashing code of LabNet with an arbitrary prior distribution. The effectiveness and efficiency of our SVHNs are verified by extensive experiments carried out on four widely-used multimedia databases, in comparison with 11 state-of-the-art approaches.},
}


@article{hu2018local,
  title={Local feature based multi-view discriminant analysis},
  author={Hu, Peng and Peng, Dezhong and Guo, Jixiang and Zhen, Liangli},
  journal={Knowledge-Based Systems},
  abbr={KBS},
  year={2018},
  pdf={KBS2018_Local_feature_based_multi-view_discriminant_analysis.pdf},
  preview={kbs18_local.png},
  abstract={In many real-world applications, an object can be represented from multiple views or styles. Thus, it is important to design algorithms that are able to recognize objects from distinct views. To the end, a large number of approaches have been proposed to achieve the heterogeneous recognition tasks through the use of local features. However, most of them only focus on binary views and thus cannot be applied to multi-view analysis. In this paper, we propose a novel local feature based multi-view discriminant analysis approach (FMDA). The proposed approach consists of three steps: First, the input images are represented using representation matrices and local feature descriptor (LFD) matrices of their overlapping patches, where the representation matrices are the linear coefficients of the LFDs for different views. In this way, it brings two advantages, i.e., addressing the small sample size (SSS) problem and preserving the discriminative information while reducing the redundant information in the LFD matrices. Second, the multi-view discriminant representation and feature projections are learned by projecting the LFDs of different views into a common space using the Fisher criterion. Finally, a simple but effective view-similarity constraint is proposed to adaptively learn the relationships between different views. To verify the effectiveness of the proposed method, extensive experiments are carried out on the FERET, CAS-PEAL-R1, CUFSF and HFB databases comparing with some state-of-the-art methods.},
}

@article{zhen2018multiobjective,
  title={Multiobjective test problems with degenerate Pareto fronts},
  author={Zhen, Liangli and Li, Miqing and Cheng, Ran and Peng, Dezhong and Yao, Xin},
  journal={arXiv preprint arXiv:1806.02706},
  abbr={arXiv},
  year={2018},
  pdf={arxiv2022_DPF.pdf},
  preview={arxiv22_dpf.png},
  code={https://liangli-zhen.github.io/code/dpf.zip},
  abstract={In multiobjective optimisation, a set of scalable test problems with a variety of features allow researchers to investigate and evaluate the abilities of different optimisation algorithms, and thus can help them to design and develop more effective and efficient approaches. Existing test problem suites mainly focus on situations where all the objectives are fully conflicting with each other. In such cases, an m-objective optimisation problem has an (m-1)-dimensional Pareto front in the objective space. However, in some optimisation problems, there may be unexpected characteristics among objectives, e.g., redundancy. The redundancy of some objectives can lead to the multiobjective problem having a degenerate Pareto front, i.e., the dimension of the Pareto front of the m-objective problem be less than (m-1). In this paper, we systematically study degenerate multiobjective problems. We abstract three general characteristics of degenerate problems, which are not formulated and systematically investigated in the literature. Based on these characteristics, we present a set of test problems to support the investigation of multiobjective optimisation algorithms under situations with redundant objectives. To the best of our knowledge, this work is the first one that explicitly formulates these three characteristics of degenerate problems, thus allowing the resulting test problems to be featured by their generality, in contrast to existing test problems designed for specific purposes (e.g., visualisation).},
}

@article{zhen2017underdetermined,
  title={Underdetermined blind source separation using sparse coding},
  author={Zhen, Liangli and Peng, Dezhong and Yi, Zhang and Xiang, Yong and Chen, Peng},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  abbr={IEEE TNNLS},
  volume={28},
  number={12},
  pages={3102--3108},
  year={2017},
  publisher={IEEE},
  preview={TNNLS17_Under.png},
  pdf={TNNLS2017_Underdetermined_blind_source_separation_using_sparse_coding.pdf},
  code={https://liangli-zhen.github.io/assets/code/UBSS-SC.zip},
  abstract={In an underdetermined mixture system with n unknown sources, it is a challenging task to separate these sources from their m observed mixture signals, where m . n. By exploiting the technique of sparse coding, we propose an effective approach to discover some 1-D subspaces from the set consisting of all the time-frequency (TF) representation vectors of observed mixture signals. We show that these 1-D subspaces are associated with TF points where only single source possesses dominant energy. By grouping the vectors in these subspaces via hierarchical clustering algorithm, we obtain the estimation of the mixing matrix. Finally, the source signals could be recovered by solving a series of least squares problems. Since the sparse coding strategy considers the linear representation relations among all the TF representation vectors of mixing signals, the proposed algorithm can provide an accurate estimation of the mixing matrix and is robust to the noises compared with the existing underdetermined blind source separation approaches. Theoretical analysis and experimental results demonstrate the effectiveness of the proposed method.},
}

@article{li2017read,
  title={How to read many-objective solution sets in parallel coordinates},
  author={Li, Miqing and Zhen, Liangli and Yao, Xin},
  journal={IEEE Computational Intelligence Magazine},
  abbr={IEEE CIM},
  volume={12},
  number={4},
  pages={88--100},
  year={2017},
  publisher={IEEE},
  preview={CIM17_How.png},
  pdf={CIM2017_How_to_read_many-objective_solution_sets_in_parallel_coordinates.pdf},
  abstract={Rapid development of evolutionary algor ithms in handling many-objective optimization problems requires viable methods of visualizing a high-dimensional solution set. The parallel coordinates plot which scales well to high-dimensional data is such a method, and has been frequently used in evolutionary many-objective optimization. However, the parallel coordinates plot is not as straightforward as the classic scatter plot to present the information contained in a solution set. In this paper, we make some observations of the parallel coordinates plot, in terms of comparing the quality of solution sets, understanding the shape and distribution of a solution set, and reflecting the relation between objectives. We hope that these observations could provide some guidelines as to the proper use of the parallel coordinates plot in evolutionary manyobjective optimization.},
}

@inproceedings{zhen2017adjusting,
  title={Adjusting parallel coordinates for investigating multi-objective search},
  author={Zhen, Liangli and Li, Miqing and Cheng, Ran and Peng, Dezhong and Yao, Xin},
  booktitle={SEAL},
  abbr={SEAL},
  pages={224--235},
  year={2017},
  pdf={APC.pdf},
  preview={SEAL17_adapt.png},
  code={https://liangli-zhen.github.io/assets/code/APC-code.zip},
  abstract={Visualizing a high-dimensional solution set over the evolution process is a viable way to investigate the search behavior of evolutionary multi-objective optimization. The parallel coordinates plot which scales well to the data dimensionality is frequently used to observe solution sets in multi-objective optimization. However, the solution sets in parallel coordinates are typically presented by the natural order of the optimized objectives, with rare information of the relation between these objectives and also the Pareto dominance relation between solutions. In this paper, we attempt to adjust parallel coordinates to incorporate this information. Systematic experiments have shown the effectiveness of the proposed method.},
}

@article{chen2017underdetermined,
  title={Underdetermined blind separation by combining sparsity and independence of sources},
  author={Chen, Peng and Peng, Dezhong and Zhen, Liangli and Luo, Yifan and Xiang, Yong},
  journal={IEEE Access},
  volume={5},
  pages={21731--21742},
  year={2017},
  abbr={IEEE Access},
  publisher={IEEE},
  preview={Access17_under.png},
  pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8081738},
  abstract={In this paper, we address underdetermined blind separation of N sources from their M instantaneous mixtures, where N > M, by combining the sparsity and independence of sources. First, we propose an effective scheme to search some sample segments with the local sparsity, which means that in these sample segments, only Q(Q <; M) sources are active. By grouping these sample segments into different sets such that each set has the same Q active sources, the original underdetermined BSS problem can be transformed into a series of locally overdetermined BSS problems. Thus, the blind channel identification task can be achieved by solving these overdetermined problems in each set by exploiting the independence of sources. In the second stage, we will achieve source recovery by exploiting a mild sparsity constraint, which is proven to be a sufficient and necessary condition to guarantee recovery of source signals. Compared with some sparsity-based UBSS approaches, this paper relaxes the sparsity restriction about sources to some extent by assuming that different source signals are mutually independent. At the same time, the proposed UBSS approach does not impose any richness constraint on sources. Theoretical analysis and simulation results illustrate the effectiveness of our approach},
}

@article{zhen2014locally,
  title={Locally linear representation for image clustering},
  author={Zhen, Liangli and Yi, Zhang and Peng, Xi and Peng, Dezhong},
  journal={Electronics Letters},
  volume={50},
  number={13},
  pages={942--943},
  year={2014},
  abbr={Electronics Letters},
  publisher={IET},
  preview={el14_locally.png},
  pdf={EL2014_Locally_linear_representation_for_image_clustering.pdf},
  abstract={The construction of the similarity graph plays an essential role in a spectral clustering (SC) algorithm. There exist two popular schemes to construct a similarity graph, i.e. the pairwise distance-based scheme (PDS) and the linear representation-based scheme (LRS). It is notable that the above schemes suffered from some limitations and drawbacks, respectively. Specifically, the PDS is sensitive to noises and outliers, while the LRS may incorrectly select inter-subspaces points to represent the objective point. These drawbacks degrade the performance of the SC algorithms greatly. To overcome these problems, a novel scheme to construct the similarity graph is proposed, where the similarity computation among different data points depends on both their pairwise distances and the linear representation relationships. This proposed scheme, called locally linear representation (LLR), encodes each data point using a collection of data points that not only produce the minimal reconstruction error but also are close to the objective point, which makes it robust to noises and outliers, and avoids selecting inter-subspaces points to represent the objective point to a large extent.},
}


@article{zhen2013local,
  title={Local neighborhood embedding for unsupervised nonlinear dimension reduction},
  author={Zhen, Liangli and Peng, Xi and Peng, Dezhong},
  journal={Journal of Software},
  abbr={J Softw},
  volume={8},
  number={2},
  pages={410--417},
  year={2013},
  publisher={Academy Publisher},
  preview={jsw13_local.png},
  pdf={JSW2013_Local_neighborhood_embedding_for_unsupervised_nonlinear_dimension_reduction.pdf},
  abstract={The construction of similarity relationship among data points plays a critical role in manifold learning. There exist two popular schemes, i.e., pairwise-distance based similarity and reconstruction coefficient based similarity. Existing works only have involved one scheme of them. These two schemes have different drawbacks. For pairwisedistance based similarity graph algorithms, they are sensitive to the noise and outliers. For reconstruction coefficient based similarity graph algorithms, they need sufficient sampled data and the neighborhood size is sensitive. This paper proposes a novel algorithm, called Local Neighborhood Embedding (LNE), which preserves pairwise-distance based similarity and reconstruction coefficient based similarity for finding the latent low dimensional structure of data. It has following three advantages: Firstly,it is insensitive to the choice of neighborhood size; Secondly, it is robust to the noise; Thirdly, It works well even in under-sampled case. Furthermore, the proposed objective function has a closedform solution, which means it has a low computational complexity, and the experimental results illustrate that LNE has a competitive performance in dimensionality reduction.},
}
